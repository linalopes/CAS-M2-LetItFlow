{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "Since RNNs are particularly useful for sequence data, we can treat the audio as a sequence of frames or windows of features extracted over time, which will allow the RNN to capture temporal dependencies in the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "We will extract sequences of features from the audio files (e.g., amplitude, spectral centroid, etc.) to feed into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the normalized audio folder\n",
    "audio_folder = '../dataset_normalized'\n",
    "\n",
    "# Function to extract sequential audio features\n",
    "def extract_sequential_features(file_path, sr=22050, n_mfcc=13):\n",
    "    y, _ = librosa.load(file_path, sr=sr)\n",
    "\n",
    "    # Extract MFCCs as a sequence of features (you can use other features as well)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Return the transposed MFCC (shape will be [sequence_length, n_mfcc])\n",
    "    return mfcc.T\n",
    "\n",
    "# Load your labels CSV\n",
    "labels_df = pd.read_csv('audio_durations_labels.csv')\n",
    "\n",
    "# Create a custom dataset class for PyTorch\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_folder, labels_df, n_mfcc=13):\n",
    "        self.audio_folder = audio_folder\n",
    "        self.labels_df = labels_df\n",
    "        self.n_mfcc = n_mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.labels_df.iloc[idx]['file_name']\n",
    "        label = self.labels_df.iloc[idx]['label']\n",
    "\n",
    "        # Map label to numerical values (short=0, medium=1, long=2)\n",
    "        label_map = {'short': 0, 'medium': 1, 'long': 2}\n",
    "        label = label_map[label]\n",
    "\n",
    "        # Load the audio file and extract sequential features (MFCCs)\n",
    "        file_path = os.path.join(self.audio_folder, file_name)\n",
    "        features = extract_sequential_features(file_path, n_mfcc=self.n_mfcc)\n",
    "\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "audio_dataset = AudioDataset(audio_folder, labels_df)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RNN Model\n",
    "We will define an RNN using nn.RNN or nn.LSTM in PyTorch, as RNNs or LSTMs are more suited to sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # Use LSTM or RNN\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # For LSTM\n",
    "\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, (h0, c0))  # Use (h0, c0) for LSTM, just h0 for RNN\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Pass the last output through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Now, weâ€™ll set up the training loop, define the loss function, and use an optimizer to train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7840\n",
      "Epoch [2/10], Loss: 0.4253\n",
      "Epoch [3/10], Loss: 0.8938\n",
      "Epoch [4/10], Loss: 0.7505\n",
      "Epoch [5/10], Loss: 0.6229\n",
      "Epoch [6/10], Loss: 0.6706\n",
      "Epoch [7/10], Loss: 0.5422\n",
      "Epoch [8/10], Loss: 1.2797\n",
      "Epoch [9/10], Loss: 0.4132\n",
      "Epoch [10/10], Loss: 0.2145\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 13  # Number of MFCC features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3  # short, medium, long\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        # Get features and labels from the batch\n",
    "        features, labels = zip(*batch)\n",
    "        features = nn.utils.rnn.pad_sequence(features, batch_first=True).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "After training, we will evaluate the RNN on the test set and calculate accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.71%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        features, labels = zip(*batch)\n",
    "        features = nn.utils.rnn.pad_sequence(features, batch_first=True).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let It Flow Web App - Export Model to use in the Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/rnn_labeled_pouring_model_lina.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_save_path = '../models/rnn_labeled_pouring_model_lina.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let It Flow Web App - Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully exported to ONNX format.\n"
     ]
    }
   ],
   "source": [
    "# Dummy input that matches the input size of your model\n",
    "dummy_input = torch.randn(1, 100, input_size).to(device)  # Example: sequence length of 100\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model,  # Your trained model\n",
    "    dummy_input,  # Dummy input to define the input shape\n",
    "    \"rnn_model.onnx\",  # Output file name\n",
    "    export_params=True,  # Store the trained parameter weights inside the model file\n",
    "    opset_version=11,  # ONNX version\n",
    "    input_names=['input'],  # Name for the input layer\n",
    "    output_names=['output'],  # Name for the output layer\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # Handle dynamic batch size\n",
    ")\n",
    "\n",
    "print(\"Model has been successfully exported to ONNX format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "rise": {
   "scroll": true,
   "transition": "convex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
